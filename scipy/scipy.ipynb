{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy\n",
    "![SciPy](https://raw.githubusercontent.com/scipy/scipy-sphinx-theme/master/_static/scipyshiny_small.png)\n",
    "\n",
    "- Uses numpy as its core\n",
    "- Numerical methods for:\n",
    "    + integration\n",
    "    + solving differential equations\n",
    "    + optimizing, minimizing \n",
    "    + root finding\n",
    "    + fast fourier transforms\n",
    "- Contains the CODATA values for many constants of nature\n",
    "- Mostly build as wrappers around time-proven fortran libraries (fftpack, lapack, fitpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Physical constants](#physical_constants)\n",
    "2. [Optimize](#optimize)\n",
    "    * [General least-squares fitting using `curve_fit`](#curve_fit)\n",
    "        - [Exercise 1](#exercise_1)\n",
    "        - [Providing uncertainties and initial guesses](#uncertainties_guesses)\n",
    "    * [Function minimization using `minimize`](#minimize)\n",
    "        - [Unbinned likelihood fits](#likelihood)\n",
    "        - [Exercise 2](#exercise_2)\n",
    "3. [Fast Fourier Transforms (FFTs)](#fft)\n",
    "4. [Integration](#integration)\n",
    "    * [Function integration](#function_integration)\n",
    "    * [Sample integration](#sample_integration)\n",
    "        - [Exercise 3](#exercise_3)\n",
    "5. [Interpolation](#interpolation)\n",
    "    * [Linear interpolation](#linear_interpolation)\n",
    "    * [Cubic spline interpolation](#spline_interpolation)\n",
    "    * [Exercise 4](#exercise_4)\n",
    "6. [Stats](#stats)\n",
    "    * [Probability distributions](#stats_distributions)\n",
    "        - [Continuous distributions](#continuous_distributions)\n",
    "        - [Discrete distributions](#discrete_distributions)\n",
    "        - [Multivariate distributions](#multivariate_distributions)\n",
    "    * [Statistical functions](#statistical_functions)\n",
    "    * [Example](#stats_example)\n",
    "    * [Exercise 5](#exercise_5)\n",
    "7. [Special Functions](#special_functions)\n",
    "    * [Signal filtering](#filtering)\n",
    "        - [Exercise 6](#exercise_6)\n",
    "    * [Bessel functions](#bessel)\n",
    "    * [Error function and Gaussian CDF](#erf)\n",
    "    * [Orthogonal Polynomials](#ortho_polys)\n",
    "        - [Exercise 7](#exercise_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup (run me first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "# we will need to plot stuff later\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=physical_constants></a>\n",
    "# Physical constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.constants as const\n",
    "const.epsilon_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert temperatures:\n",
    "const.convert_temperature(100, old_scale='C', new_scale='K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# more constants (including units and errors)!\n",
    "\n",
    "for k, v in const.physical_constants.items():\n",
    "    print(k, ':', v)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "const?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, unit, uncertainty = const.physical_constants['muon mass energy equivalent in MeV']\n",
    "\n",
    "val, unit, uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=optimize></a>\n",
    "# Optimize\n",
    "\n",
    "<a id=curve_fit></a>\n",
    "## General least-squares fitting using `curve_fit`\n",
    "\n",
    "Non-linear least-squares with Levenberg-Marquardt numerical minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -1\n",
    "b = 5\n",
    "c = 2\n",
    "\n",
    "def f(x, a, b):\n",
    "    return np.exp(a * x) + b\n",
    "\n",
    "def g(x, a, b, c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "y = f(x, a, b) + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "plt.plot(x, y, '.', label='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "params, covariance_matrix = curve_fit(g, x, y)\n",
    "\n",
    "uncertainties = np.sqrt(np.diag(covariance_matrix))\n",
    "error = ((y - g(x, *params))**2).sum()\n",
    "\n",
    "print('a = {:5.2f} ± {:.2f}'.format(params[0], uncertainties[0]))\n",
    "print('b = {:5.2f} ± {:.2f}'.format(params[1], uncertainties[1]))\n",
    "print('c = {:5.2f} ± {:.2f}'.format(params[2], uncertainties[2]))\n",
    "print('error: {}'.format(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x, g(x, *params), label='fit result')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this method to do a linear regression\n",
    "\n",
    "def f_linear(x, a, b, c):\n",
    "    return a*x + b\n",
    "\n",
    "l_params, l_covariance_matrix = curve_fit(f_linear, x, y)\n",
    "l_uncertainties = np.sqrt(np.diag(l_covariance_matrix))\n",
    "\n",
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x, f_linear(x, *l_params), label='fit result')\n",
    "plt.legend();\n",
    "\n",
    "error = ((y - f_linear(x, *l_params))**2).sum()\n",
    "print('error: {}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check that the result is the same as the one obtained from the linear regression method\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "print('With curve_fit')\n",
    "print('\\ta = {:5.2f} ± {:.2f}'.format(l_params[0], l_uncertainties[0]))\n",
    "print('\\tb = {:5.2f} ± {:.2f}'.format(l_params[1], l_uncertainties[1]))\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "print('With linregress')\n",
    "print('\\ta = {:5.2f}'.format(slope))\n",
    "print('\\tb = {:5.2f}'.format(intercept))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise_1></a>\n",
    "## Exercise 1\n",
    "\n",
    "Use the `curve_fit` function to fit a quadratic polynomial function to the data, plot the result and compare the error with the previous fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=uncertainties_guesses></a>\n",
    "### Providing uncertainties and initial guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "def f(x, a, b):\n",
    "    return np.sin(a * x + b)\n",
    "\n",
    "y = f(x, 5 * np.pi, np.pi / 2) \n",
    "yerr = np.full_like(y, 0.2)\n",
    "noise = np.random.normal(0, yerr, 1000)\n",
    "y += noise\n",
    "\n",
    "\n",
    "#params, covariance_matrix = curve_fit(f, x, y)\n",
    "\n",
    "#params, covariance_matrix = curve_fit(\n",
    "#    f, x, y,\n",
    "#    p0=[10, 2],\n",
    "#)\n",
    "\n",
    "params, covariance_matrix = curve_fit(\n",
    "    f, x, y,\n",
    "    p0=[15, 1.5],\n",
    "    sigma=yerr,\n",
    "    absolute_sigma=True,\n",
    ")\n",
    "\n",
    "\n",
    "# plot the stuff\n",
    "\n",
    "x_plot = np.linspace(-0.1, 1.1, 1000)\n",
    "\n",
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x_plot, f(x_plot, *params), label='fit result')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=minimize></a>\n",
    "## Unconstraint function minimization using `minimize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function with two local minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x, h, x1):\n",
    "    return h*np.exp(-((x - x1)**2).sum(axis=-1))\n",
    "\n",
    "def g(x):\n",
    "    return -f(x, 1, np.array([2, 1])) - f(x, 2, np.array([-2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XLIM = [-5, 5]\n",
    "YLIM = [-2.5, 7.5]\n",
    "x = np.linspace(*XLIM, 100)\n",
    "y = np.linspace(*YLIM, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "Z = g(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(X, Y, Z, cmap='viridis_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to find the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = (-2, 0)\n",
    "r = minimize(g, x0=x0)\n",
    "\n",
    "plt.contourf(X, Y, Z, cmap='viridis_r')\n",
    "plt.colorbar()\n",
    "plt.plot([r.x[0]], [r.x[1]], 'ro-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't found the absolute minimum\n",
    "\n",
    "Let's see the path followed by minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global min_path\n",
    "\n",
    "min_path = [(4., 1.)]\n",
    "\n",
    "def store_path(x):\n",
    "    min_path.append(x)\n",
    "\n",
    "r = minimize(g, x0=min_path[0], callback=store_path, tol=1e-20, method='CG',\n",
    "             options={'gtol': 1e-10, 'maxiter':100, 'eps': 1e-9})\n",
    "\n",
    "plt.contourf(X, Y, Z, cmap='viridis_r')\n",
    "plt.colorbar()\n",
    "plt.plot([p[0] for p in min_path], [p[1] for p in min_path], 'ro-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where do different initial guesses lead to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_range = np.linspace(-8, 8, 33)\n",
    "y_range = np.linspace(-4, 10, 29)\n",
    "\n",
    "X, Y = np.meshgrid(x_range, y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0_arr = np.vstack([X.reshape(-1), Y.reshape(-1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_g(x):\n",
    "    return minimize(g, x0=x).x\n",
    "\n",
    "result = np.apply_along_axis(minimize_g, 1, x0_arr) # providing an initial guess is mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = np.isclose(result, [-2, 3])\n",
    "s2 = np.isclose(result, [2, 1])\n",
    "s3 = ~(np.logical_or(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x0_arr[s1[:, 0], 0], x0_arr[s1[:, 0], 1], 'bo')\n",
    "plt.plot(x0_arr[s2[:, 0], 0], x0_arr[s2[:, 0], 1], 'ro')\n",
    "plt.plot(x0_arr[s3[:, 0], 0], x0_arr[s3[:, 0], 1], c='0.9', marker='o', linestyle='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=likelihood></a>\n",
    "### Unbinned likelihood fits\n",
    "\n",
    "Example: an unbinned negative log-likelihood fit for a poissonian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 15\n",
    "k = np.random.poisson(lambda_, 1000)\n",
    "\n",
    "# make sure to use bins of integer width, centered around the integer\n",
    "bin_edges = np.arange(0, 31) - 0.5\n",
    "\n",
    "plt.hist(k, bins=bin_edges);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson pdf:\n",
    "\n",
    "$$ \n",
    "f(k, \\lambda) = \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda}\n",
    "$$\n",
    "\n",
    "So the likelihood is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i=0}^{N} \\frac{\\lambda^{k_i}}{k_i!} \\mathrm{e}^{-\\lambda}\n",
    "$$\n",
    "\n",
    "It's often easier to minimize $-\\log(\\mathcal{L})$, let's see:\n",
    "\n",
    "$$\n",
    "-\\log(\\mathcal{L}) = - \\sum_{i=0}^{N}\\bigl( k_i \\log(\\lambda) - \\log{k_i!} - \\lambda \\bigr)\n",
    "$$\n",
    "\n",
    "We are interested in the minimum relative to $\\lambda$, so we dismiss constant term not concerning $\\lambda$ \n",
    "$$\n",
    "-\\log(\\mathcal{L}) = \\sum_{i=0}^{N}\\bigl( \\lambda - k_i \\log(\\lambda) \\bigr)   \n",
    "$$\n",
    "\n",
    "This looks indeed easier to minimize than the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(lambda_, k):\n",
    "    return np.sum(lambda_ - k * np.log(lambda_))\n",
    "\n",
    "result = minimize(\n",
    "    negative_log_likelihood,\n",
    "    x0=(10, ),   # initial guess\n",
    "    args=(k, ),  # additional arguments for the function to minimize\n",
    ")\n",
    "\n",
    "result\n",
    "\n",
    "print('True λ = {}'.format(lambda_))\n",
    "print('Fit: λ = {:.2f} ± {:.2f}'.format(result.x[0], np.sqrt(result.hess_inv[0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* minimize has lots of options for different minimization algorithms\n",
    "* Also able to respect bounds and constraints (with certain algorithms)\n",
    "* It is worth to write down you problems and simplify the (log)Likelihood as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise_2></a>\n",
    "### Exercise 2\n",
    "\n",
    "Do the same to estimate the parameters of a gaussian distribution.\n",
    "\n",
    "Generate a sample of a normal distribution with $\\mu = 10$ and $\\sigma = 6$\n",
    "\n",
    "pdf:\n",
    "\n",
    "$$\n",
    "f(x, \\mu, \\sigma) =  \\frac{1}{\\sqrt{2 \\pi}} \\mathrm{e}^{-0.5 \\frac{(x - \\mu)^2}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Minimize the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "-\\log(\\mathcal{L}) = -\\sum_{i = 0}^N \\log\\bigl( \\frac{1}{\\sqrt{2 \\pi}} \\mathrm{e}^{-0.5 \\frac{(x_i - \\mu)^2}{\\sigma^2}}  \\bigr)\n",
    "$$\n",
    "\n",
    "You can make use of the `norm.rvs` and `norm.pdf` functions to generate the sample and define the function to be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=fft></a>\n",
    "# Fast Fourier Transforms (FFTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq1 = 5\n",
    "freq2 = 50\n",
    "\n",
    "t = np.linspace(0, 1, 1024*10)\n",
    "y = np.sin(2*np.pi*freq1*t) + np.sin(2*np.pi*freq2*t)\n",
    "\n",
    "# add some white noise\n",
    "noisy_y = y + np.random.normal(np.full_like(y, 0.), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t, noisy_y, s=10, alpha=0.25, lw=0)\n",
    "plt.plot(t, y, c='k')\n",
    "plt.xlabel(r'$t \\ /\\ \\mathrm{s}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import fftpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = fftpack.rfft(noisy_y)\n",
    "f = fftpack.rfftfreq(len(t), t[1] - t[0])\n",
    "\n",
    "plt.axvline(freq1, color='lightgray', lw=5)\n",
    "plt.axvline(freq2, color='lightgray', lw=5)\n",
    "\n",
    "plt.plot(f, z**2)\n",
    "\n",
    "plt.xlabel('f / Hz')\n",
    "plt.xscale('log')\n",
    "# plt.yscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=integration></a>\n",
    "# Integration\n",
    "\n",
    "Scipy integration routines are discussed in the [Scipy documentation](https://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html). We will look at the two most common routines here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=function_integration></a>\n",
    "## Function integration\n",
    "\n",
    "`quad` is used to evaluate definite 1D numerical integrals using a technique from the Fortran library QUADPACK.\n",
    "\n",
    "For example, assume we want to integrate a quadratic polynomial $f(x) = 3x^2 + 6x - 9$ over an interval $x \\in [0, 5]$. Analytically, the answer is:\n",
    "\n",
    "$$ \\int_0^5 3x^2 + 6x - 9 \\ dx = \\left[ x^3 + 3x^2 - 9x \\right]_{x = 0}^{x = 5} = 155 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 + 6*x - 9\n",
    "\n",
    "quad(f, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter `quad` returns is the answer; the second is an estimate of the absolute error in the result.\n",
    "\n",
    "For 2D, 3D, or n-dimensional integrals , use `dblquad`, `tplquad`, or `nquad`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some more interesting functions, Scipy's other function integration routines might be helpful:\n",
    "* `quadrature` : [Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature)\n",
    "* `romberg` : [Romberg integration](https://en.wikipedia.org/wiki/Romberg%27s_method)\n",
    "\n",
    "For example, consider the $\\mathrm{sinc}$ function:\n",
    "\n",
    "$$\n",
    "\\mathrm{sinc}(x) \\equiv\n",
    "\\begin{cases} \n",
    "1 & x = 0 \\\\\n",
    "\\sin(x)/x & \\mathrm{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.sinc(x)\n",
    "plt.plot(x, y)\n",
    "plt.title('Sinc Function')\n",
    "\n",
    "res = quad(np.sinc, -10, 10)\n",
    "\n",
    "plt.text(-10, 0.8, r'$ \\int_{{-10}}^{{10}} \\mathrm{{sinc}}(x) \\ dx = {result:.4f}$ ?'.format(result=res[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`quad` struggles with $\\mathrm{sinc}$, but it can be easily handled with Gaussian quadrature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quadrature\n",
    "\n",
    "# quadrature may complain, but it will work in the end\n",
    "print(quadrature(np.sinc, -10, 10)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result agrees with Mathematica to 13 decimal places (even though only 11 are shown). Note that the problem is the singularity at $x=0$; if we change the boundaries to, say, [-10.1, 10], then it works fine. Also, writing our sinc function more cleverly would eliminate the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=sampleintegration></a>\n",
    "## Sample integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a collection of points that you want to integrate, you could use an [interpolation function](#interpolation) and pass it to `quad`. A better alternative is to use the purpose-built functions `trapz`, `romb`, and `simps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the $\\mathrm{sinc}$ function again as an example. The most naive (and surprisingly robust) integration method is using the trapazoid rule, which is implemented in `trapz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import trapz\n",
    "\n",
    "# 50 grid points\n",
    "x1 = np.linspace(-10, 10, 51)\n",
    "y1 = np.sinc(x1)\n",
    "print('  50 points:', trapz(y1, x1))   # note the order of the arguments: y, x\n",
    "\n",
    "# 1000 grid points\n",
    "x = np.linspace(-10, 10, 10000)\n",
    "y = np.sinc(x)\n",
    "print('1000 points:', trapz(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise_3></a>\n",
    "### Exercise 3\n",
    "\n",
    "Apply the `trapz` function to calculate:\n",
    "\n",
    "$$\n",
    "\\int_{-4}^{4} \\sqrt[3]{(1 - x^3)} dx\n",
    "$$\n",
    "\n",
    "**Hint** use the `np.cbrt` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=interpolation></a>\n",
    "# Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=linear_interpolation></a>\n",
    "## Linear interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you would like to interpolate between two points $(x_0, y_0)$ and $(x_1, y_1)$. You could do this by hand:\n",
    "\n",
    "$$y(x) = y_0 + (x - x_0) \\frac{y_1 - y_0}{x_1 - x_0}$$\n",
    "\n",
    "Simple enough, but it is annoying to look up or derive the formula. Also, what if you want values less than $x_0$ to stay at the value of $y_0$, and likewise for values greater than $x_1$? Then you need to add `if` statements, and check the logic, etc. Too much work.\n",
    "\n",
    "Instead, there is a simple function for almost all of your interpolation needs: `interp1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "x = (1, 2)\n",
    "y = (5, 7)\n",
    "print('Points:', list(zip(x, y)))\n",
    "\n",
    "f = interp1d(x, y)\n",
    "z = [1.25, 1.5, 1.75]\n",
    "print('Interpolation:', list(zip(z, f(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, if you try to use an x-coordinate outside of the interval $[x_0, x_1]$, a `ValueError` will be raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(2.5)   # uncomment to run me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we haven't told `interp1d` how we want to handle the boundaries. This is done using the `fill_value` keyword argument. There are a few options:\n",
    "\n",
    "1. Set values outside of the interval $[x_0, x_1]$ to a float.\n",
    "2. Set values $< x_0$ to `below` and values $> x_1$ to `above` by passing a tuple, `(below, above)`.\n",
    "3. Extrapolate points outside the interval by passing `extrapolate`.\n",
    "\n",
    "We also need to tell `interp1d` not to raise a `ValueError` by setting the `bounds_error` keyword to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [0.5, 1, 1.5, 2, 2.5]\n",
    "\n",
    "f = interp1d(x, y, bounds_error=False, fill_value=0)\n",
    "print(\"Option 1:\", list(zip(z, f(z))))\n",
    "\n",
    "f = interp1d(x, y, bounds_error=False, fill_value=y)   # fill with endpoint values\n",
    "print(\"Option 2:\", list(zip(z, f(z))))\n",
    "\n",
    "f = interp1d(x, y, fill_value='extrapolate')   # bounds_error set to False automatically\n",
    "print(\"Option 3:\", list(zip(z, f(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=spline_interpolation></a>\n",
    "## Cubic spline interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cubic splines](http://mathworld.wolfram.com/CubicSpline.html) are what are most commonly used when you want to interpolate between points *smoothly*.\n",
    "\n",
    "Cubic spline interpolation is so common, it has its own method, `CubicSpline`, which produces generally better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# Generate a sample of the sinus function\n",
    "sample_x = np.linspace(-np.pi, np.pi, 10)\n",
    "sample_y = np.sin(sample_x)\n",
    "\n",
    "# Sinus function\n",
    "z = np.linspace(np.min(sample_x), np.max(sample_x), 100)\n",
    "plt.plot(z, np.sin(z), label='real')\n",
    "\n",
    "# 1D interpolation\n",
    "f1 = interp1d(sample_x, sample_y)\n",
    "plt.plot(z, f1(z), label='interp1d')\n",
    "\n",
    "# The cubic spline\n",
    "f2 = CubicSpline(sample_x, sample_y)\n",
    "plt.plot(z, f2(z), label='CubicSpline')\n",
    "\n",
    "plt.plot(sample_x, sample_y, 'ko')\n",
    "plt.legend();\n",
    "plt.grid(linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interpolate on 2 and N dimensional spaces (with some restrictions).\n",
    "See [the scipy docs on interpolate](https://docs.scipy.org/doc/scipy/reference/interpolate.html#module-scipy.interpolate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise_4></a>\n",
    "## Exercise 4\n",
    "\n",
    "Interpolate the $sinc$ function we have already seen, using:\n",
    "* two samples of 10 and 100 points\n",
    "* linear interpolation and cubic splines\n",
    "* on the [-10, 10] interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=stats></a>\n",
    "# Stats\n",
    "\n",
    "The scipy.stats module provides mainly:\n",
    "* probability distributions: continuous, discrete and multivariate\n",
    "* statistical functions such as statistics and tests\n",
    "\n",
    "For further details you can check [the official documentation](https://docs.scipy.org/doc/scipy/reference/stats.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=stats_distributions></a>\n",
    "## Probability distributions\n",
    "\n",
    "The scipy.stats module provides a very complete set of probability distributions.\n",
    "\n",
    "There are three types of distributions:\n",
    "* Continuous\n",
    "* Discrete\n",
    "* Multivariate\n",
    "\n",
    "Each of the univariate types is inherited from the same class, so they all have a common API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=continuous_distributions></a>\n",
    "### Continuos distributions\n",
    "\n",
    "There are ~100 different continuous distributions. Some of the methods in the API:\n",
    "* `cdf`: Cumulative Distribution Function\n",
    "* `pdf`: Probability Density Function\n",
    "* `rvs`: Random Variable Sample\n",
    "* `ppf`: Percent Point Function (inverse of the CDF)\n",
    "* `fit`: return MLE estimations of location, scale and shape, given a set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_normal = stats.norm()\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, std_normal.pdf(x))\n",
    "plt.title('Standard Normal - Probability Density Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(r'$ f(x) =  \\frac{1}{\\sqrt{2 \\pi}} \\mathrm{e}^{-\\frac{1}{2} x^2}}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, std_normal.cdf(x))\n",
    "plt.title('Standard Normal - Cumulative Distribution Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(r'$ F(x) =  \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{x}\\mathrm{e}^{-\\frac{1}{2} x^2}}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = std_normal.rvs(100)\n",
    "hist_result = plt.hist(x_sample, range=[-3, 3], bins=100, normed=True)\n",
    "x_plot = np.linspace(-3, 3, 100)\n",
    "plt.plot(x_plot, std_normal.pdf(x_plot))\n",
    "plt.title('Standard Normal - Random Sample')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the estimation of the mean and the standard deviation from a sample\n",
    "stats.norm.fit(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "\n",
    "# Name of the probability distribution, class, range of values to plot\n",
    "pds = [('Normal', stats.norm(), (-4., 4.)), \n",
    "      ('LogNormal', stats.lognorm(1.), (0., 4.)),\n",
    "      ('Students T', stats.t(3.), (-10., 10.)),\n",
    "      ('Chi Squared', stats.chi2(1.), (0., 10.))]\n",
    "\n",
    "n_pds = len(pds)\n",
    "fig, ax_list = plt.subplots(n_pds, 3, figsize=(5.*n_pds, 10.))\n",
    "for ind, elem in enumerate(pds):\n",
    "    \n",
    "    pd_name, pd_func, pd_range = elem\n",
    "\n",
    "    x_range = np.linspace(*pd_range, 101)\n",
    "    \n",
    "    # Probability Density Function\n",
    "    ax_list[ind, 0].plot(x_range, pd_func.pdf(x_range))\n",
    "    ax_list[ind, 0].set_ylabel(pd_name)\n",
    "    \n",
    "    # Cumulative Distribution Function\n",
    "    ax_list[ind, 1].plot(x_range, pd_func.cdf(x_range))\n",
    "    ax_list[ind, 1].fill_between(x_range, pd_func.cdf(x_range))\n",
    "    ax_list[ind, 1].set_ylim([0., 1.])\n",
    "    \n",
    "    # Random Variable Sample\n",
    "    ax_list[ind, 2].hist(pd_func.rvs(size=N_SAMPLES), bins=50)\n",
    "    \n",
    "    if ind == 0:\n",
    "        _ = ax_list[ind, 0].set_title('Probability Density Function')\n",
    "        _ = ax_list[ind, 1].set_title('Cumulative Distribution Function')\n",
    "        _ = ax_list[ind, 2].set_title('Random Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=discrete_distributions></a>\n",
    "### Discrete Distributions\n",
    "Discrete distributions have quite the same API. Having pmf= Probability Mass Function (instead of pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "\n",
    "pds = [('Binomial', stats.binom(20, 0.7), (0., 21.)),\n",
    "      ('Poisson', stats.poisson(10.), (0., 21.))]\n",
    "\n",
    "n_pds = len(pds)\n",
    "fig, ax_list = plt.subplots(n_pds, 3)\n",
    "fig.set_size_inches((8.*n_pds, 8.))\n",
    "for ind, elem in enumerate(pds):\n",
    "    \n",
    "    pd_name, pd_func, pd_range = elem\n",
    "\n",
    "    x_range = np.arange(*pd_range)\n",
    "    \n",
    "    # Probability Mass Function\n",
    "    ax_list[ind, 0].bar(x_range, pd_func.pmf(x_range))\n",
    "    ax_list[ind, 0].set_ylabel(pd_name)\n",
    "    \n",
    "    # Cumulative Distribution Function\n",
    "    ax_list[ind, 1].plot(x_range, pd_func.cdf(x_range))\n",
    "    ax_list[ind, 1].fill_between(x_range, pd_func.cdf(x_range))\n",
    "    ax_list[ind, 1].set_ylim([0., 1.])\n",
    "    \n",
    "    # Random Variable Sample\n",
    "    ax_list[ind, 2].hist(pd_func.rvs(size=N_SAMPLES), bins=x_range - 0.5)\n",
    "    \n",
    "    if ind == 0:\n",
    "        _ = ax_list[ind, 0].set_title('Probability Mass Function')\n",
    "        _ = ax_list[ind, 1].set_title('Cumulative Distribution Function')\n",
    "        _ = ax_list[ind, 2].set_title('Random Sample')\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=multivariate_distributions></a>\n",
    "### Multivariate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a multivariate normal distribution\n",
    "mult_mean = [0.1, 2.]\n",
    "mult_cov =  [[2.0, 0.3], [0.3, 0.5]]\n",
    "mult_norm = stats.multivariate_normal(mean=mult_mean, cov=mult_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "XLIM = [-6., 6.]\n",
    "YLIM = [0., 4.]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16., 8.))\n",
    "\n",
    "# Make data.\n",
    "x = np.linspace(*XLIM, 100)\n",
    "y = np.linspace(*YLIM, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "Z = mult_norm.pdf(pos)\n",
    "S = mult_norm.rvs(1000)\n",
    "\n",
    "color_norm = colors.Normalize(vmin=0, vmax=0.5)\n",
    "\n",
    "# Contour plot of the PDF\n",
    "ax1.contourf(X, Y, Z, cmap='coolwarm', norm=color_norm)\n",
    "ax1.set_title('Probability Density Function')\n",
    "\n",
    "\n",
    "# 2D histogram of the random sample\n",
    "h = ax2.hist2d(S[:, 0], S[:, 1], bins=50, normed=True, \n",
    "               cmap='coolwarm', range=[XLIM, YLIM],\n",
    "              norm=color_norm)\n",
    "ax2.set_title('Random sample')\n",
    "\n",
    "# Add a common color bar\n",
    "fig.colorbar(h[3], ax=[ax1, ax2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=statistical_functions></a>\n",
    "## Statistical Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality test\n",
    "\n",
    "Used to determine if a sample comes from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_sample = stats.norm().rvs(size=1000)\n",
    "\n",
    "alpha = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(x_sample, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality test using `normaltest`: Tests if a sample comes from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_result(p, alpha):\n",
    "    print(\"p = {:g}\".format(p))\n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k2, p = stats.normaltest(x_sample)\n",
    "print_result(p, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test\n",
    "\n",
    "To test if a sample matches a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = x_sample.mean()\n",
    "s = x_sample.std(ddof=1)\n",
    "print('loc={} scale={}'.format(l, s))\n",
    "\n",
    "k, p = stats.kstest(x_sample, stats.norm(l, s).cdf)\n",
    "print_result(p, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And much more: descriptive functions, more tests, contingency tables,...\n",
    "\n",
    "See [the docs](https://docs.scipy.org/doc/scipy/reference/stats.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=stats_example></a>\n",
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_prices = pd.read_csv('resources/stock.csv')\n",
    "df_prices.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = df_prices[['Apple', 'Microsoft']].plot(title='2016 stock prices', figsize=(6., 6.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the daily relative increments\n",
    "df_incs = df_prices.drop('Date', axis=1)\n",
    "df_incs = ((df_incs - df_incs.shift(1))/df_incs.shift(1)).loc[1:, :]\n",
    "df_incs['Date'] = df_prices.Date\n",
    "df_incs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12., 6.))\n",
    "_ = df_incs[['Apple', 'Microsoft']].plot(ax=ax1)\n",
    "_ = ax2.scatter(df_incs['Apple'], df_incs['Microsoft'])\n",
    "ax2.grid()\n",
    "ax2.set_xlabel('Apple')\n",
    "ax2.set_ylabel('Microsoft')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `fit` method to MLE of the mean and the std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can use the fit method to get the MLE of the mean and the std\n",
    "p = stats.norm.fit(df_incs.Apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create estimated distributions based on the sample\n",
    "app_dist = stats.norm(*p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can test if this data fits a normal distribution (Kolmogorov-Smirnov test)\n",
    "app_K, app_p = stats.kstest(df_incs['Apple'], app_dist.cdf)\n",
    "print_result(app_p, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise_5></a>\n",
    "## Exercise 5\n",
    "\n",
    "Do the same for Microsoft:\n",
    "* Fit a normal distribution to the data\n",
    "* Create the instance of the Normal distribution\n",
    "* Test if the sample comes from this normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done this. Here comes the tougher part:\n",
    "\n",
    "Imagine you are a product designer in a finantial company. You want to create a new investment product to be \"sold\" to your clients based on the future stock prices of some IT companies. The profit the client gets from his investement is calculated like this:\n",
    "* At  the time of the investment we check the initial stock prices of Apple and Microsoft\n",
    "* 12 months later (let's say 240 work days), the client gets 100% of the investement back. Additionally if all stock prices are higher than the initial ones, the client earns half the lowest increment (in %). \n",
    "\n",
    "**What is the expected profit of this product?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=special_functions></a>\n",
    "# Special Functions\n",
    "\n",
    "A complete list of scipy special functions can be found [here](https://docs.scipy.org/doc/scipy-0.14.0/reference/special.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=filtering></a>\n",
    "# Signal filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this noisy data set with outliers. The data is a so-called S-curve, and we want to identify the midpoint of the falling edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import ndtr\n",
    "\n",
    "def s_curve(x, a, b):\n",
    "    return ndtr(-a*(x - b))\n",
    "\n",
    "# generate mildly noisy data using Gaussian CDF (see end of this notebook)\n",
    "real_params = [2.5, 3]\n",
    "x = np.linspace(0, 5, 20)\n",
    "y = s_curve(x, *real_params)\n",
    "y += np.random.normal(0, 0.025, len(y))\n",
    "\n",
    "# add 4 bad data points\n",
    "outlier_xcoords = [2, 6, 10, 15]\n",
    "y[outlier_xcoords] = np.random.uniform(0.2, 2, size=4)\n",
    "plt.plot(x, y, 'bo')\n",
    "\n",
    "# attempt to fit\n",
    "params, __ = curve_fit(s_curve, x, y)\n",
    "z = np.linspace(0, 5, 100)\n",
    "plt.plot(z, s_curve(z, *params), 'k--')\n",
    "print('Real value:', real_params[1])\n",
    "print('Fit value:', params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see clearly in the data that the mid-point of the S-curve is at about x=3 (which is the real value), but the outliers destroy the fit. We can remove them easily with a median filter. A median filter is particularly suited to edge detection cases, since it tends to preserve edges well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "filtered_y = medfilt(y)\n",
    "\n",
    "params, __ = curve_fit(s_curve, x, filtered_y)\n",
    "print('Real value:', real_params[1])\n",
    "print('Fit value:', params[1])\n",
    "\n",
    "z = np.linspace(0, 5, 100)\n",
    "plt.plot(x, y, 'k*', label='Before Filtering')\n",
    "plt.plot(x, filtered_y, 'bo', label='After Filtering')\n",
    "plt.plot(z, s_curve(z, *params), 'g--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise_6></a>\n",
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example implementation of a low-pass [Butterworth filter](https://en.wikipedia.org/wiki/Butterworth_filter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order=5):\n",
    "    \"\"\"\n",
    "    Digital Butterworth low-pass filter.\n",
    "    \n",
    "    data   : 1D array of data to be filtered\n",
    "    cutoff : cutoff frequency in Hz\n",
    "    fs     : sampling frequency (samples/second)\n",
    "    \"\"\"\n",
    "    nyquist_frequency = fs/2\n",
    "    normal_cutoff = cutoff/nyquist_frequency\n",
    "    b, a = butter(order, normal_cutoff, btype='low')\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are the unfortunate recipient of the following noisy data, which contains noise at two different (unknown) frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('resources/scipy_filter_data.dat')\n",
    "t = data[:, 0]\n",
    "y = data[:, 1]\n",
    "sample_freq = (len(t) - 1)/(t[-1])\n",
    "plt.plot(t, y);   # these are your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhere in this mess is a Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def gaussian(x, mu, sigma, A):\n",
    "    return A * norm.pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a FFT to identify the two offending noise frequencies. Then convert the `lowpass_filter` above into a bandstop filter (hint: it is a trivial modification), and remove the offending noise from the data as much as possible (it won't be perfect). Finally, use `curvefit` to fit a Gaussian to the data, thereby recovering the original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bessel></a>\n",
    "## Bessel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import jn\n",
    "\n",
    "x = np.linspace(0, 10, 100)\n",
    "for n in range(6):\n",
    "    plt.plot(x, jn(n, x), label=r'$\\mathtt{J}_{%i}(x)$' % n)\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as plt3d\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def airy_disk(x):\n",
    "    mask = x != 0\n",
    "    result = np.empty_like(x)\n",
    "    result[~mask] = 1.0\n",
    "    result[mask] = (2 * jn(1, x[mask]) / x[mask])**2\n",
    "    return result\n",
    "\n",
    "# 2D plot\n",
    "r = np.linspace(-10, 10, 500)\n",
    "plt.plot(r, airy_disk(r))\n",
    "\n",
    "# 3D plot\n",
    "x = np.arange(-10, 10.1, 0.1)\n",
    "y = np.arange(-10, 10.1, 0.1)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = airy_disk(np.sqrt(X**2 + Y**2))\n",
    "\n",
    "result\n",
    "fig = plt.figure()\n",
    "ax = plt3d.Axes3D(fig)\n",
    "ax.plot_surface(X, Y, Z, cmap='gray', norm=LogNorm(), lw=0)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=erf></a>\n",
    "## Error function and Gaussian CDF\n",
    "\n",
    "CDF = cumulative distribution function\n",
    "\n",
    "$$\\mathrm{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z \\exp\\left( -t^2 \\right) dt $$\n",
    "$$\\mathrm{ndtr}(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z \\exp\\left( \\frac{-t^2}{2} \\right) dt $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import erf, ndtr\n",
    "\n",
    "def gaussian(z):\n",
    "    return np.exp(-z**2)\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, gaussian(x), label='Gaussian')\n",
    "plt.plot(x, erf(x), label='Error Function')\n",
    "plt.plot(x, ndtr(x), label='Gaussian CDF')\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=ortho_polys></a>\n",
    "## Orthogonal Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import eval_legendre, eval_laguerre, eval_hermite, eval_chebyt\n",
    "\n",
    "ortho_poly_dict = {'Legendre': eval_legendre,\n",
    "                   'Laguerre': eval_laguerre,\n",
    "                   'Hermite': eval_hermite,\n",
    "                   'Chebyshev T': eval_chebyt}\n",
    "\n",
    "def plot_ortho_poly(name):\n",
    "    plt.figure()\n",
    "    f = ortho_poly_dict[name]\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    for n in range(5):\n",
    "        plt.plot(x, f(n, x), label='n = %i' % n)\n",
    "    if name is 'Legendre' or 'Chebyshev' in name:\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "    plt.legend(loc='best', fontsize=16)\n",
    "    plt.title(name + ' Polynomials')\n",
    "    \n",
    "plot_ortho_poly('Legendre')\n",
    "# plot_ortho_poly('Laguerre')\n",
    "# plot_ortho_poly('Hermite')\n",
    "# plot_ortho_poly('Chebyshev T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercise_7></a>\n",
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonal polynomials can be used to construct a series expansion of an arbitrary function, just like $\\sin$ and $\\cos$ are used to construct a Fourier series. For example, we can express a function $f(x)$ as a series of Legendre polynomials $P_n(x)$:\n",
    "\n",
    "$$ f(x) = \\sum_{n=0}^{\\infty} a_n P_n(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Legendre polynomials are orthogonal on the interval $x \\in [-1, 1]$, where they obey the following orthogonality relationship:\n",
    "$$ \\int_{-1}^{1} P_n(x) \\, P_m(x) \\, dx = \\frac{2}{2 m + 1} \\delta_{mn} $$\n",
    "\n",
    "With $f(x) = sin(\\pi x)$, write a function to calculate the coefficients $a_n$ of the Legendre series. Then plot $f(x)$ and the Legendre series for $x \\in [-1, 1]$. Calculate as many coefficients as are needed for the series to essentially the same as $f(x)$ (it will be less than ten).\n",
    "\n",
    "If you are struggling with the math, look [here](http://mathworld.wolfram.com/Fourier-LegendreSeries.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "\n",
    "#### Fitting with (i)minuit\n",
    "\n",
    "One of the most extended minimization packages classically used for fitting is [MINUIT](https://en.wikipedia.org/wiki/MINUIT). Python allows to use this software via wrapping the C++ code into a python package named [iminuit](https://github.com/iminuit/iminuit).\n",
    "\n",
    "For more information see [iminuit documentation](http://iminuit.readthedocs.io/en/latest/), and the [available tutorials](https://github.com/iminuit/iminuit/blob/master/tutorial/tutorial.py) within the GitHub project.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
